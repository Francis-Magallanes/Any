{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<style>\n",
    ".figure{    \n",
    "    text-align:center;\n",
    "}\n",
    ".figure img{\n",
    "    height : 250px;\n",
    "    width : 500 px;\n",
    "}\n",
    "\n",
    ".equation{\n",
    "    text-align:center;\n",
    "}\n",
    "\n",
    "</style>\n",
    "\n",
    "## Laboratory Activity 3\n",
    "\n",
    "by Francis John Magallanes and John Matthew Vong\n",
    "\n",
    "\n",
    "### Theory\n",
    "A perceptron is a single layer neural network with no hidden layers [[1]](https://deepai.org/machine-learning-glossary-and-terms/perceptron). In the figure below, it shows a neural network with one neuron (thus, it can be called as a perceptron). A neural network  consists of neurons. These neurons will consist (mathematically) of weights, biases, and an activation funcion as seen in the figure below. Under the topic of activation function, there are many activation function to choose from. One of the examples is the sigmoid function. To learn more about different activiation function, go to this [link](https://www.upgrad.com/blog/types-of-activation-function-in-neural-networks/). Note that 'X' represent the inputs of the neuron, 'W' represent the weights of the neuron, 'B' represent the bias and the f(x*W^T + b) represents the activation function [[2](https://www.geeksforgeeks.org/introduction-to-artificial-neutral-networks/)].  \n",
    "\n",
    "<div class = \"figure\"> \n",
    "<img src=\"https://media.geeksforgeeks.org/wp-content/uploads/single_neuron-1.jpg\"/>\n",
    "<p>Figure 1: Graphical Representation of a Neuron (Credits to geeksforgeeks)</p>\n",
    "</div>\n",
    "\n",
    "\n",
    "These components of a neuron will be used in the two algorithms invovled in neural networks namely:  **Feed Forward** and **Backpropagation**.  \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### *Feed Forward*\n",
    "Feed forward describes the linear flow of the input to the output without having loops within the neural network. With this feed forward, the input value can be transformed into corresponding output. The math behind this feed forward is the sum of the bias and product of the inputs and the weights. These sum will be then feed to the activation function which denoted by f(). The output of the activation function is the output of a single neuron. Equation 1 shows this mathematically based on figure 1.\n",
    "\n",
    "<br>\n",
    "\\begin{equation}\n",
    "A = f(Z + b)\n",
    "\\end{equation}\n",
    "\\begin{equation}\n",
    "Z = w1*x1 + w2*x2  + w3*x3\n",
    "\\end{equation}\n",
    "Where $A$ is the output of the neuron\n",
    "<p class = \"equation\">Equation 1: Output of the Neuron</p>\n",
    "<br>\n",
    "\n",
    "This mathematical form can be expressed using the matrices and dot product. To use the dot product, express the weights in terms of row vector and the input as the column vector as seen in equation 2 and 3. With equations 2 and 3, the output of the neuron can be rewritten as seen in equation 4 [[4](https://brilliant.org/wiki/feedforward-neural-networks/)].\n",
    "\n",
    "<br>\n",
    "\\begin{equation}\n",
    "w = \n",
    "\\begin{bmatrix}\n",
    "x1&x2&x3\n",
    "\\end{bmatrix}\n",
    "\\end{equation}\n",
    "<p class = \"equation\">Equation 2: Matrix of the Weights</p>\n",
    "<br>\n",
    "\n",
    "<br>\n",
    "\\begin{equation}\n",
    "inputs = \n",
    "\\begin{bmatrix}\n",
    "x1\\\\x2\\\\x3\\\\\n",
    "\\end{bmatrix}\n",
    "\\end{equation}\n",
    "<p class = \"equation\">Equation 3: Matrix of the Inputs</p>\n",
    "<br>\n",
    "\n",
    "<br>\n",
    "\\begin{equation}\n",
    "A = f(Z + b)\n",
    "\\end{equation}\n",
    "\\begin{equation}\n",
    "Z = w \\cdot inputs\n",
    "\\end{equation}\n",
    "<p class = \"equation\">Equation 4: Output of the neuron in terms of Matrices</p>\n",
    "<br>\n",
    "\n",
    "Note that this weight matrices can be extended as seen in equation 5 as the numbers of the neurons increases. Same with the inputs in equation 5. Also, the bias can be represented as a matrix to represent the biases of different neurons in a layer.\n",
    "\n",
    "\n",
    "<br>\n",
    "\\begin{equation}\n",
    "inputs = \n",
    "\\begin{bmatrix}\n",
    "x1\\\\x2\\\\x3\\\\...\\\\xN\\\\\n",
    "\\end{bmatrix}\n",
    "\\end{equation}\n",
    "\n",
    "\\begin{equation}\n",
    "w = \n",
    "\\begin{bmatrix}\n",
    "x11&x12&x13&...&x1N\\\\\n",
    "x21&x22&x23&...&x2N\\\\\n",
    ".&.&.&...&.\\\\\n",
    ".&.&.&...&.\\\\\n",
    ".&.&.&...&.\\\\\n",
    "xM(N-3)&xM(N-2)&xM(N-1)&...&xMN\\\\\n",
    "\\end{bmatrix}\n",
    "\\end{equation}\n",
    "\n",
    "\\begin{equation}\n",
    "b = \n",
    "\\begin{bmatrix}\n",
    "b1\\\\b2\\\\b3\\\\.\\\\.\\\\.\\\\bN\\\\\n",
    "\\end{bmatrix}\n",
    "\\end{equation}\n",
    "<p class = \"equation\">Equation 5: Extension of weights matrices,inputs matrices, and biases</p>\n",
    "<br>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### *Backpropagation*\n",
    "In neural networks, the weights and biases will be adjusted so that the output of the neural network will have an output close to the expected output. The event adjustment of the weights and biases is called \"Training\". One of the ways to adjust the weights and biases of a neural network is through the algorithm of Backpropagation. Backpropagation \"is the step where we sen the error to update the weights\". Backpropagation involves the concept of cost function. This cost function is a way to quantify the difference between the output of the neural network and the expected output [[5]](https://www.jeremyjordan.me/neural-networks-training/). One of the examples of the cost function is the sum of mean squared errors. With this cost function, it will help in defining how much is to adjust in the weights and biases through the use of gradient descent concept(which involves partial derivaties and chain rule). The math behind the backpropagation is very long and tedious. To learn about the derivation and its mathematical concept behind, visit this [website](https://www.jeremyjordan.me/neural-networks-training/). To make it short, the equation 6 and equation 7 below shows the updating the weights and biases respectively. \n",
    "\n",
    "<br>\n",
    "\\begin{equation}\n",
    "\\omega^{n} =  \\omega^{n} - \\eta \\frac{\\partial C}{\\partial w^n}\n",
    "\\end{equation}\n",
    "Where: $\\omega^n$ is the weight in the n-th layer, $\\eta$ is the learning rate, and $C$ is the cost function.\n",
    "<p class = \"equation\">Equation 6: Formula for Updating the Weights</p>\n",
    "\n",
    "<br>\n",
    "\\begin{equation}\n",
    "b^{n} =  b^{n} - \\eta \\frac{\\partial C}{\\partial b^n}\n",
    "\\end{equation}\n",
    "Where: $b^n$ is the bias in the n-th layer, $\\eta$ is the learning rate, and $C$: is the cost function\n",
    "<p class = \"equation\">Equation 7: Formula for Updating the bias</p>\n",
    "\n",
    "<br>To find the partial derivatives portion of the equation 7, below shows the different equivalences of the partial derivatives.\n",
    "<br>\n",
    "\n",
    "$$\n",
    "\\frac {\\partial C}{\\partial A^n}: given\n",
    "$$  \n",
    "$$\n",
    "\\frac {\\partial C}{\\partial Z^n} = f'(Z^n)  \\frac {\\partial C}{\\partial A^n}\n",
    "$$  \n",
    "$$\n",
    "\\frac {\\partial C}{\\partial w^n} = \\frac {\\partial C}{\\partial Z^n}.({A^{n-1}})^T\n",
    "$$  \n",
    "$$\n",
    "\\frac {\\partial C}{\\partial b^n} = \\frac {\\partial C}{\\partial Z^n}\n",
    "$$  \n",
    "$$\n",
    "\\frac {\\partial C}{\\partial A^{n-1}} = {({w^n})}^T . \\frac {\\partial C}{\\partial Z^{n}}\n",
    "$$  \n",
    "<p class = \"equation\">Equation 8: For Updating the weight and bias</p>\n",
    "\n",
    "With the equations above, the steps for the \"training the neural network\" is as follows [[5](https://www.jeremyjordan.me/neural-networks-training/)]: \n",
    "\n",
    "1. Perform feed forward\n",
    "2. Compute for $\\frac {\\partial C}{\\partial A^n}$ from the output layer.\n",
    "3. Compute all partial derivatives (for that specific layer) stated in equation 8 based on the results of number 2\n",
    "4. Update the respective weights and biases based on the computed values\n",
    "5. Go back one layer\n",
    "6. Repeat 2 through 4 until it reaches to the input layer\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Other references:\n",
    "\n",
    "* Neural Network from Scratch.ipynb by Adarsh Menon. Here is the [link](https://colab.research.google.com/drive/1ku3LvrqovKOzeCTkW6bLHYxNKMaG4KFv#scrollTo=nteMKEk2Qylh)\n",
    "\n",
    "* Neural Networks from Scartch in Python(Playlist from youtube) by Adarsh Menon. Here is the [link](https://youtube.com/playlist?list=PLP3ANEJKF1TwHRDS9sPANOzYaAIfJIuam) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import time\n",
    "\n",
    "#definition of the activation function and its derivative\n",
    "def sigmoid(x):\n",
    "    return 1/(1+np.exp(-x))\n",
    "\n",
    "#derivative of the sigmoid function\n",
    "def d_sigmoid(x):\n",
    "    return sigmoid(x)*(1-sigmoid(x))\n",
    "\n",
    "#definition of the cost function\n",
    "#squared error\n",
    "def square_error(y_actual, y_expected):\n",
    "    return (1/2)*np.square(np.subtract(y_actual,y_expected))\n",
    "\n",
    "#derivative of the squared error\n",
    "def d_square_error(y_actual, y_expected):\n",
    "    return np.subtract(y_actual,y_expected)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#definition of the layer class so that the multi-layered neural network can be implemented\n",
    "\n",
    "class Layer:\n",
    "    #this will use a sigmoid activation function\n",
    "    \n",
    "\n",
    "    def __init__(self,number_inputs, number_neurons):\n",
    "\n",
    "        # initializing the weights with random numbers between 0 to 1\n",
    "        self.weights = np.random.randn(number_neurons, number_inputs) \n",
    "\n",
    "        #initialize the bias with one\n",
    "        self.bias = np.ones((number_neurons,1))\n",
    "\n",
    "        #default learning rate is 0.5. Don't change these value pls.\n",
    "        #use the training method to change the learning rate\n",
    "        self.learning_rate = 0.5 \n",
    "\n",
    "    def feedforward(self,inputs):\n",
    "        #apply feedforward algorithm with equation 4 here\n",
    "        self.previous_output = inputs\n",
    "        self.Z = np.dot(self.weights,inputs) + self.bias\n",
    "        self.A = sigmoid(self.Z)\n",
    "        return self.A\n",
    "        \n",
    "    def backpropagate(self, dC_dA):\n",
    "        #applying the formulas for updatting of the weights and biases\n",
    "        #refer to equation 8\n",
    "        dC_dZ = np.multiply(d_sigmoid(self.Z),dC_dA) \n",
    "\n",
    "        #normalize the values\n",
    "        #since the dot product and the np.sum is summed values from the samples.\n",
    "        # With this code below, the results of the dot product and the summed values\n",
    "        #will be average, respectively\n",
    "        dC_dW = 1/ dC_dZ.shape[1] * np.dot(dC_dZ,self.previous_output.T)\n",
    "\n",
    "        #np.sum is necessary to make the shape of the dC_dB and dC_dZ same\n",
    "        #np.sum finding the sum along the column matrix to reduce it one column matrix.\n",
    "        #it sums across the samples\n",
    "        dC_dB =  1/ dC_dZ.shape[1] * np.sum(dC_dZ, axis=1,keepdims=True)\n",
    "        #print(\"dC_dW : {}, dC_dB: {}, dC_dZ: {}\". format(dC_dW,dC_dB,dC_dZ)) # uncomment this to see the respectively values\n",
    "\n",
    "        #update the weights and biases using equation 7\n",
    "        self.weights = self.weights - np.multiply(self.learning_rate,dC_dW)\n",
    "        self.bias = self.bias - np.multiply(self.learning_rate,dC_dB)\n",
    "\n",
    "        #return this value for the backpropagation of the different layer\n",
    "        return np.dot(self.weights.T,dC_dZ)\n",
    "\n",
    "## TO DO : Add export and import files\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#With the layer class, it will provide an abstraction to neural network for organization\n",
    "\n",
    "class NeuralNetwork:\n",
    "\n",
    "    # Documentation:\n",
    "    #neurons_per_layer is a list (or a scalar value) that tells the neurons per layer.\n",
    "    #Each row values will contain the number of neurons for that layer\n",
    "    #for example [1,2,3], this will create a three layers of neural network\n",
    "    #with the 1 neurons in the first layer, 2 neurons in the second layer,\n",
    "    #3 neurons in the third layer \n",
    "    #number_inputs is number of inputs to be inputted in the neural network \n",
    "    def __init__(self,number_inputs,neurons_per_layer):\n",
    "        #concatenate the number_inputs in the first index of the neurons_per_layer\n",
    "        temp = np.concatenate(([number_inputs], neurons_per_layer))\n",
    "\n",
    "        #create a list of layers\n",
    "        self.layers = [ Layer(temp[i], temp[i+1]) for i in range(temp.shape[0] - 1)]\n",
    "    \n",
    "    def train(self, x_train, y_train, learning_rate = 0.5, epochs = 10000):\n",
    "\n",
    "        if learning_rate != 0.5:\n",
    "            #change the learning rate accrodingly for each of the layers\n",
    "            for layer in self.layers:\n",
    "                layer.learning_rate = learning_rate\n",
    "        \n",
    "        #TRAINING SESSION PROPER\n",
    "        for epoch in range(epochs):\n",
    "\n",
    "            #feedforward\n",
    "            input = x_train\n",
    "            for layer in self.layers:\n",
    "                input = layer.feedforward(input)\n",
    "            \n",
    "            #print(\"Input: {}\".format(input))\n",
    "\n",
    "            #calculation of the derivative of the cost based on the input variable \n",
    "            dCost = d_square_error(input,y_train)\n",
    "\n",
    "            #uncomment this to know the cost for each of the epoch\n",
    "            #size_y_train = y_train.shape[0]\n",
    "            #cost = (1/size_y_train) * square_error(input,y_train).sum()\n",
    "            #print(\"Output: {} Cost: {}\".format(input,cost)) #uncomment this show the output\n",
    "\n",
    "            #backpropagation\n",
    "            for layer in reversed(self.layers):\n",
    "                dCost = layer.backpropagate(dCost)\n",
    "\n",
    "    def predict(self,x_predict):\n",
    "        #implements the feed forward\n",
    "        y_predict = x_predict\n",
    "        for layer in self.layers:\n",
    "                y_predict = layer.feedforward(y_predict)\n",
    "        \n",
    "        return y_predict\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#With the defined neural network class, instantiated a neural network with input layer (4 inputs), 2 hidden layer(with 3  neurons and 2 neurons, respectively), and output layer (with 1 neurons)\n",
    "neural_network = NeuralNetwork(4,[784,784,1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#TRAINING SESSION\n",
    " #possible inputs based on the activity\n",
    " #with dimensions of (number of inputs) by (number of training samples) - col x row\n",
    " #size: 4 x 8\n",
    "x_train = np.array([[1,1,0,0,1,0,0,0],[0,0,0,1,1,0,0,0],[0,0,1,0,0,1,0,1],[1,0,1,0,0,1,1,0]])\n",
    "\n",
    " #possible outputs based on the activity\n",
    "#with dimensions of (number of outputs) by (number of training samples) - col x row\n",
    "#size: 1 x 8\n",
    "y_train = np.array([[1,1,0,0,1,1,0,0]])\n",
    "\n",
    "start = time.time()\n",
    "neural_network.train(x_train, y_train, learning_rate = 0.9, epochs = 20000)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "[[0.98861908 0.98694134 0.49988989 0.00992675 0.98718651 0.49988989\n  0.01757157 0.01509214]]\nTime training: 2.9207663536071777\n"
     ]
    }
   ],
   "source": [
    "#for the predictions\n",
    "\n",
    "#for the calculating the time of training\n",
    "time_training = time.time() - start\n",
    "\n",
    "#input for the neural network\n",
    "x_test = np.array([[1,1,0,0,1,0,0,0],[0,0,0,1,1,0,0,0],[0,0,1,0,0,1,0,1],[1,0,1,0,0,1,1,0]])\n",
    "\n",
    "y_test = neural_network.predict(x_test)\n",
    "\n",
    "#show the output\n",
    "print(y_test)\n",
    "print(\"Time training: {}\".format(time_training))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "name": "python378jvsc74a57bd0ce11f34a6e98b7940ddd1de6bba18d8b39c708accd7fa23783d62410ed992f80",
   "display_name": "Python 3.7.8 64-bit"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.8-final"
  },
  "metadata": {
   "interpreter": {
    "hash": "ce11f34a6e98b7940ddd1de6bba18d8b39c708accd7fa23783d62410ed992f80"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}